{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99d1c8c3-9a86-4edd-94d3-ad0e59c2757a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# UDF Distribution Test: Git Repo Code to Workers\n",
    "\n",
    "## Test Plan:\n",
    "1. **Test 1:** Show that imports from Git Repo FAIL on workers (without addPyFile)\n",
    "2. **Test 2:** Fix with addPyFile and show it works\n",
    "3. **Test 3:** Monitor which nodes execute the UDFs\n",
    "\n",
    "**Prerequisites:**\n",
    "- Cluster with at least 1 driver + 1 worker\n",
    "- Git Repo with `taxi_udfs/calculations.py` module\n",
    "\n",
    "**Update REPO_PATH below to match your workspace path!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0188cb0d-418d-4417-9d2e-b02783337928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration - UPDATE THIS PATH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89338eea-851b-451f-a845-7d14915cea24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC1 Using repo path: /Workspace/Users/sarbani.maiti@databricks.com/dab-mlops-demo-4dec/mlops-udf-demo\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# UPDATE THIS PATH TO YOUR REPO LOCATION IN DATABRICKS\n",
    "# ============================================================\n",
    "REPO_PATH = \"/Workspace/Users/sarbani.maiti@databricks.com/dab-mlops-demo-4dec/mlops-udf-demo\"\n",
    "\n",
    "print(f\"Using repo path: {REPO_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "391ae28e-23d4-4ff1-9467-550894987b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Cluster & Environment Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48fbf90-02ec-45f0-8a18-f4ac964217c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nCLUSTER INFORMATION\n============================================================\nDriver Hostname: 1218-095811-m484hsz3-10-0-9-182\nSpark Version: 3.5.2\nNumber of Workers: 2\nDefault Parallelism: 8\nApplication ID: app-20251210073449-0000\n============================================================\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import os\n",
    "\n",
    "# Print cluster info\n",
    "print(\"=\" * 60)\n",
    "print(\"CLUSTER INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Driver Hostname: {socket.gethostname()}\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# Get executor count (subtract 1 for driver)\n",
    "try:\n",
    "    num_executors = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "    print(f\"Number of Workers: {num_executors}\")\n",
    "except:\n",
    "    print(\"Number of Workers: Unable to determine\")\n",
    "\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store driver hostname for later comparison\n",
    "DRIVER_HOSTNAME = socket.gethostname()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45100d70-19e7-4247-b9c5-d32af1fd88a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create Test Data (Force Distribution to Workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595c55f6-d041-4711-a837-57021e2fb529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created DataFrame with 10000 rows\n✅ Partitioned into 16 partitions\n✅ Data will be processed across worker nodes\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "# Create enough data to force distribution across workers\n",
    "# 10K rows ensures data is spread across partitions/workers\n",
    "data = [(i, float(i % 50 + 5), float((i % 100) + 10), float(i % 20), i % 24, i % 2 == 0) \n",
    "        for i in range(10000)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"trip_id\", IntegerType(), False),\n",
    "    StructField(\"distance\", DoubleType(), False),\n",
    "    StructField(\"fare\", DoubleType(), False),\n",
    "    StructField(\"tip\", DoubleType(), False),\n",
    "    StructField(\"hour\", IntegerType(), False),\n",
    "    StructField(\"is_weekend\", BooleanType(), False),\n",
    "])\n",
    "\n",
    "# Repartition to ensure parallel execution across workers\n",
    "num_partitions = max(spark.sparkContext.defaultParallelism * 2, 8)\n",
    "test_df = spark.createDataFrame(data, schema).repartition(num_partitions)\n",
    "\n",
    "print(f\"✅ Created DataFrame with {test_df.count()} rows\")\n",
    "print(f\"✅ Partitioned into {test_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"✅ Data will be processed across worker nodes\")\n",
    "\n",
    "# Cache for repeated tests\n",
    "test_df.cache()\n",
    "test_df.count()  # Force cache materialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46111b56-dc68-441b-b11b-1ae660d4aedb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Fix with spark.sparkContext.addPyFile() using ZIP\n",
    "\n",
    "This test shows how to properly distribute Git Repo code to workers.\n",
    "\n",
    "\n",
    " **ZIP the module** and add the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdcb85f-1267-4eef-bd74-7404c5ea6e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Creating zip file of taxi_udfs module...\n   Source: /Workspace/Users/sarbani.maiti@databricks.com/dab-mlops-demo-4dec/mlops-udf-demo/taxi_udfs/\n   Destination: /tmp/taxi_udfs.zip\n\n   Added: taxi_udfs/__init__.py\n   Added: taxi_udfs/calculations.py\n\n✅ Created zip file: /tmp/taxi_udfs.zip\n   Contents: ['taxi_udfs/__init__.py', 'taxi_udfs/calculations.py']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create a zip file containing the taxi_udfs module\n",
    "ZIP_PATH = \"/tmp/taxi_udfs.zip\"\n",
    "\n",
    "print(\"\uD83D\uDD04 Creating zip file of taxi_udfs module...\")\n",
    "print(f\"   Source: {REPO_PATH}/taxi_udfs/\")\n",
    "print(f\"   Destination: {ZIP_PATH}\")\n",
    "print(\"\")\n",
    "\n",
    "# Create the zip file\n",
    "with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add __init__.py\n",
    "    init_path = f\"{REPO_PATH}/taxi_udfs/__init__.py\"\n",
    "    zipf.write(init_path, \"taxi_udfs/__init__.py\")\n",
    "    print(f\"   Added: taxi_udfs/__init__.py\")\n",
    "    \n",
    "    # Add calculations.py\n",
    "    calc_path = f\"{REPO_PATH}/taxi_udfs/calculations.py\"\n",
    "    zipf.write(calc_path, \"taxi_udfs/calculations.py\")\n",
    "    print(f\"   Added: taxi_udfs/calculations.py\")\n",
    "\n",
    "print(\"\")\n",
    "print(f\" Created zip file: {ZIP_PATH}\")\n",
    "\n",
    "# List contents of zip to verify\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as zipf:\n",
    "    print(f\"   Contents: {zipf.namelist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f78eb4a-6b8a-4f9e-b571-530d579703a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Add the ZIP file to Spark context using addPyFile() - this distributes to ALL workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59408f5-ce29-4ffd-8bfe-5cc49c55b65f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added zip file to Spark context\n✅ Module will be distributed to all worker nodes\n\n\uD83D\uDCDD Workers can now import using: from taxi_udfs.calculations import ...\n"
     ]
    }
   ],
   "source": [
    "# Add the ZIP file to Spark context - this distributes to ALL workers\n",
    "spark.sparkContext.addPyFile(ZIP_PATH)\n",
    "\n",
    "print(\"✅ Added zip file to Spark context\")\n",
    "print(\"✅ Module will be distributed to all worker nodes\")\n",
    "print(\"\")\n",
    "print(\"\uD83D\uDCDD Workers can now import using: from taxi_udfs.calculations import ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d15e88d5-cbdb-4247-8b48-9ef853599f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define UDFs to  work on workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98199bd-3814-4313-b21e-ea67e5230fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ UDFs defined successfully with ZIP + addPyFile fix\n\n\uD83D\uDCDD Key insight:\n   - We zipped the module WITH its package structure (taxi_udfs/)\n   - So we can use the full import: from taxi_udfs.calculations import ...\n   - This matches how it works on the driver!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "import socket\n",
    "import os\n",
    "\n",
    "# UDF 1: Calculate fare per mile\n",
    "@udf(returnType=FloatType())\n",
    "def fare_per_mile_udf(fare, distance):\n",
    "    \"\"\"\n",
    "    Calculate fare per mile using the distributed module.\n",
    "    Since we zipped with package structure (taxi_udfs/), \n",
    "    we import using the full package path.\n",
    "    \"\"\"\n",
    "    from taxi_udfs.calculations import calculate_fare_per_mile\n",
    "    return calculate_fare_per_mile(fare, distance)\n",
    "\n",
    "# UDF 2: Categorize fare\n",
    "@udf(returnType=StringType())\n",
    "def categorize_fare_udf(fare):\n",
    "    \"\"\"Categorize fare amount into price tiers.\"\"\"\n",
    "    from taxi_udfs.calculations import categorize_fare_amount\n",
    "    return categorize_fare_amount(fare)\n",
    "\n",
    "# UDF 3: Calculate tip percentage\n",
    "@udf(returnType=FloatType())\n",
    "def tip_percentage_udf(tip, fare):\n",
    "    \"\"\"Calculate tip as percentage of fare.\"\"\"\n",
    "    from taxi_udfs.calculations import calculate_tip_percentage\n",
    "    return calculate_tip_percentage(tip, fare)\n",
    "\n",
    "# UDF 4: Track execution location (which worker runs this)\n",
    "@udf(returnType=StringType())\n",
    "def get_execution_location():\n",
    "    \"\"\"\n",
    "    Returns information about WHERE this UDF is executing.\n",
    "    This helps verify that UDFs run on workers, not just the driver.\n",
    "    \"\"\"\n",
    "    hostname = socket.gethostname()\n",
    "    pid = os.getpid()\n",
    "    executor_id = os.environ.get(\"SPARK_EXECUTOR_ID\", \"driver\")\n",
    "    return f\"host={hostname}|pid={pid}|executor={executor_id}\"\n",
    "\n",
    "print(\"✅ UDFs defined successfully with ZIP + addPyFile fix\")\n",
    "print(\"\")\n",
    "print(\"\uD83D\uDCDD Key insight:\")\n",
    "print(\"   - We zipped the module WITH its package structure (taxi_udfs/)\")\n",
    "print(\"   - So we can use the full import: from taxi_udfs.calculations import ...\")\n",
    "print(\"   - This matches how it works on the driver!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3eab0d4-bf63-46ca-bdaa-fec9eb816624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execute the UDFs - THIS SHOULD WORK NOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c3e541-ed13-4024-90f1-4662c4135588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Executing UDFs on workers...\n\n✅ SUCCESS! Processed 10000 rows\n\n\uD83D\uDCCA Sample Results (first 10 rows):\n----------------------------------------------------------------------------------------------------\nTrip ID          Fare   Distance     $/Mile Category         Tip% Executed On\n----------------------------------------------------------------------------------------------------\n1814       $   24.00       19.0 $    1.26 expensive       58.3% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n2029       $   39.00       34.0 $    1.15 expensive       23.1% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n1675       $   85.00       30.0 $    2.83 premium         17.6% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n1321       $   31.00       26.0 $    1.19 expensive        3.2% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n1127       $   37.00       32.0 $    1.16 expensive       18.9% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n1073       $   83.00       28.0 $    2.96 premium         15.7% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n1306       $   16.00       11.0 $    1.45 moderate        37.5% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n1518       $   28.00       23.0 $    1.22 expensive       64.3% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n1425       $   35.00       30.0 $    1.17 expensive       14.3% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n1487       $   97.00       42.0 $    2.31 premium          7.2% host=1218-095811-m484hsz3-10-0-11-143|pid=11710|executor=driver\n----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD04 Executing UDFs on workers...\")\n",
    "print(\"\")\n",
    "\n",
    "result_df = test_df.select(\n",
    "    col(\"trip_id\"),\n",
    "    col(\"fare\"),\n",
    "    col(\"distance\"),\n",
    "    col(\"tip\"),\n",
    "    fare_per_mile_udf(col(\"fare\"), col(\"distance\")).alias(\"fare_per_mile\"),\n",
    "    categorize_fare_udf(col(\"fare\")).alias(\"fare_category\"),\n",
    "    tip_percentage_udf(col(\"tip\"), col(\"fare\")).alias(\"tip_pct\"),\n",
    "    get_execution_location().alias(\"executed_on\")\n",
    ")\n",
    "\n",
    "# Force execution with collect()\n",
    "results = result_df.collect()\n",
    "\n",
    "print(f\"✅ SUCCESS! Processed {len(results)} rows\")\n",
    "print(\"\")\n",
    "print(\"\uD83D\uDCCA Sample Results (first 10 rows):\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Trip ID':<10} {'Fare':>10} {'Distance':>10} {'$/Mile':>10} {'Category':<12} {'Tip%':>8} {'Executed On'}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for row in results[:10]:\n",
    "    print(f\"{row.trip_id:<10} ${row.fare:>8.2f} {row.distance:>10.1f} ${row.fare_per_mile:>8.2f} {row.fare_category:<12} {row.tip_pct:>7.1f}% {row.executed_on}\")\n",
    "\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4229fb2-91e4-4f82-9f15-5403004d5c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## TEST 3: Monitor Which Workers Executed the UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "413967fe-cc92-48a1-9ce1-eab0fb7ec4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n\uD83D\uDCCA UDF EXECUTION DISTRIBUTION ACROSS NODES\n======================================================================\n\n+---------------------------------------------------------------+-----+\n|executed_on                                                    |count|\n+---------------------------------------------------------------+-----+\n|host=1218-095811-m484hsz3-10-0-0-109|pid=12352|executor=driver |1250 |\n|host=1218-095811-m484hsz3-10-0-0-109|pid=12350|executor=driver |1250 |\n|host=1218-095811-m484hsz3-10-0-0-109|pid=12335|executor=driver |1250 |\n|host=1218-095811-m484hsz3-10-0-0-109|pid=12344|executor=driver |1250 |\n|host=1218-095811-m484hsz3-10-0-11-143|pid=12618|executor=driver|1250 |\n|host=1218-095811-m484hsz3-10-0-11-143|pid=12620|executor=driver|1250 |\n|host=1218-095811-m484hsz3-10-0-11-143|pid=12614|executor=driver|1250 |\n|host=1218-095811-m484hsz3-10-0-11-143|pid=12615|executor=driver|1250 |\n+---------------------------------------------------------------+-----+\n\n\uD83D\uDCC8 Execution Statistics:\n   Total rows processed: 10000\n   Unique executors used: 8\n\n\uD83D\uDCCD Executor Details:\n   • Hostname: 1218-095811-m484hsz3-10-0-0-109\n     PID: 12352, Executor ID: driver\n\n   • Hostname: 1218-095811-m484hsz3-10-0-0-109\n     PID: 12350, Executor ID: driver\n\n   • Hostname: 1218-095811-m484hsz3-10-0-0-109\n     PID: 12344, Executor ID: driver\n\n   • Hostname: 1218-095811-m484hsz3-10-0-0-109\n     PID: 12335, Executor ID: driver\n\n   • Hostname: 1218-095811-m484hsz3-10-0-11-143\n     PID: 12620, Executor ID: driver\n\n   • Hostname: 1218-095811-m484hsz3-10-0-11-143\n     PID: 12615, Executor ID: driver\n\n   • Hostname: 1218-095811-m484hsz3-10-0-11-143\n     PID: 12614, Executor ID: driver\n\n   • Hostname: 1218-095811-m484hsz3-10-0-11-143\n     PID: 12618, Executor ID: driver\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze execution distribution across workers\n",
    "print(\"=\" * 70)\n",
    "print(\"\uD83D\uDCCA UDF EXECUTION DISTRIBUTION ACROSS NODES\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "\n",
    "# Group by execution location to see distribution\n",
    "execution_stats = result_df.groupBy(\"executed_on\").count().orderBy(\"count\", ascending=False)\n",
    "execution_stats.show(truncate=False)\n",
    "\n",
    "# Get detailed stats\n",
    "unique_executors = [row.executed_on for row in execution_stats.collect()]\n",
    "total_tasks = sum(row[\"count\"] for row in execution_stats.collect())\n",
    "\n",
    "print(f\"\uD83D\uDCC8 Execution Statistics:\")\n",
    "print(f\"   Total rows processed: {total_tasks}\")\n",
    "print(f\"   Unique executors used: {len(unique_executors)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"\uD83D\uDCCD Executor Details:\")\n",
    "for exec_info in unique_executors:\n",
    "    # Parse executor info\n",
    "    parts = dict(p.split(\"=\") for p in exec_info.split(\"|\"))\n",
    "    print(f\"   • Hostname: {parts.get('host', 'N/A')}\")\n",
    "    print(f\"     PID: {parts.get('pid', 'N/A')}, Executor ID: {parts.get('executor', 'N/A')}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de941510-d0ed-4569-81d4-9d321750c33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n\uD83C\uDFAF WORKER EXECUTION VERIFICATION\n======================================================================\n\nDriver hostname: 1218-095811-m484hsz3-10-0-9-182\n\nRows processed on DRIVER: 0\nRows processed on WORKERS: 10000\n\n✅ CONFIRMED: UDFs are executing on WORKER NODES!\n   Worker hosts: {'1218-095811-m484hsz3-10-0-11-143', '1218-095811-m484hsz3-10-0-0-109'}\n   100.0% of work done on workers\n"
     ]
    }
   ],
   "source": [
    "# Check if execution happened on workers (not just driver)\n",
    "print(\"=\" * 70)\n",
    "print(\"\uD83C\uDFAF WORKER EXECUTION VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "print(f\"Driver hostname: {DRIVER_HOSTNAME}\")\n",
    "print(\"\")\n",
    "\n",
    "# Count executions on driver vs workers\n",
    "driver_count = 0\n",
    "worker_count = 0\n",
    "worker_hosts = set()\n",
    "\n",
    "for row in execution_stats.collect():\n",
    "    exec_info = row.executed_on\n",
    "    count = row[\"count\"]\n",
    "    if DRIVER_HOSTNAME in exec_info:\n",
    "        driver_count += count\n",
    "    else:\n",
    "        worker_count += count\n",
    "        parts = dict(p.split(\"=\") for p in exec_info.split(\"|\"))\n",
    "        worker_hosts.add(parts.get('host', 'unknown'))\n",
    "\n",
    "print(f\"Rows processed on DRIVER: {driver_count}\")\n",
    "print(f\"Rows processed on WORKERS: {worker_count}\")\n",
    "print(\"\")\n",
    "\n",
    "if worker_count > 0:\n",
    "    print(\"✅ CONFIRMED: UDFs are executing on WORKER NODES!\")\n",
    "    print(f\"   Worker hosts: {worker_hosts}\")\n",
    "    pct = (worker_count / total_tasks) * 100\n",
    "    print(f\"   {pct:.1f}% of work done on workers\")\n",
    "elif driver_count > 0 and worker_count == 0:\n",
    "    print(\"⚠️  All execution happened on the DRIVER\")\n",
    "    print(\"   Possible reasons:\")\n",
    "    print(\"   - Single-node cluster (no workers)\")\n",
    "    print(\"   - Data too small / not enough partitions\")\n",
    "    print(\"   - Local mode execution\")\n",
    "else:\n",
    "    print(\"❓ Unable to determine execution location\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "UDF_Distribution_Test (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}